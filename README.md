## NoCode-bench â€” Frontend

The **NoCode-bench Frontend** is a browser-based user interface for interacting with the NoCode-bench evaluation system.

It enables users to define **documentation-driven feature addition tasks**, trigger **automated evaluations**, and inspect **generated code patches and evaluation results** through a unified web interface.

## Links & Deployment

- **Live Demo**: https://no-code-bench-frontend.vercel.app  
- **Deployment**: Hosted on Vercel  
- **Backend**: Connected to the NoCode-bench Evaluation API  

## Supported Task Types

The frontend supports two evaluation modes:

- **Verified NoCode-bench Benchmark Tasks**  
  Curated tasks derived from real-world documentation changes with predefined test suites.

- **Custom GitHub Feature Requests**  
  User-defined feature addition tasks on arbitrary public GitHub repositories.

No local setup or configuration is required.  
All interactionsâ€”from task specification to result inspectionâ€”are performed directly in the browser.

## 1. Platform Overview

NoCode-bench is a **web-based evaluation platform for documentation-driven code changes**.

The frontend serves as the primary interaction layer, allowing users to:

- Specify a feature addition task using natural language
- Trigger a fully automated evaluation workflow
- Inspect generated code changes and evaluation outcomes

From the userâ€™s perspective, the system abstracts away:

- Repository cloning
- Code modification
- Test execution

Users focus only on **task definition** and **result interpretation**, while all execution logic is handled by the backend.

## 2. Typical User Flow (Quick Guide)

### Step 1: Home Page

- Start from the landing page.
- Click **â€œDefine a Feature Addition Taskâ€** to begin.

### Step 2: Choose Task Type

Users can select one of the following task specification modes:

#### Option A: Built-in Verified Tasks

- Choose from curated NoCode-bench benchmark instances.
- Tasks are derived from real-world documentation changes.
- No additional input is required.

#### Option B: Custom GitHub Repository

- Provide a **GitHub repository URL**.
- Describe the intended feature change in **natural language**.

### Step 3: Run Evaluation

- Click **â€œRun Task Evaluationâ€**.
- The frontend submits the task to the backend.
- Execution typically takes **10â€“20 minutes**, depending on:
  - Repository size
  - Test suite complexity
- A loading view displays:
  - Current task status
  - Elapsed execution time


### Step 4: View Results

Once execution completes, users are redirected to the results page, where they can:

- Inspect the generated code patch (diff view)
  - ğŸŸ¢ Green lines indicate added code
  - ğŸ”´ Red lines indicate removed code
- Review evaluation outcomes and metrics based on test execution


### Step 5: Exit or Restart

- Users may return to the home page to define and run another task.


## 3. Project Structure Highlights

```text
src/
â”œâ”€â”€ pages/                 # Application pages
â”‚   â”œâ”€â”€ home.js             # Landing page
â”‚   â”œâ”€â”€ chooseRepo.js       # Task specification (verified & custom)
â”‚   â”œâ”€â”€ singleRepo.js       # Verified task selection & execution
â”‚   â””â”€â”€ status.js           # Evaluation results & patch viewer
â”‚
â”œâ”€â”€ styles/
â”‚   â”œâ”€â”€ NoCodeBench.css     # Main styling
â”‚   â””â”€â”€ App.css
â”‚
â”œâ”€â”€ App.js                  # Main routing setup
â”œâ”€â”€ index.js                # Application entry point
â””â”€â”€ index.css  
```

## 4. Notes & Limitations
* Evaluation tasks may take 10â€“20 minutes depending on repository size and test complexity.
* Results depend on the quality of existing tests in the target repository.